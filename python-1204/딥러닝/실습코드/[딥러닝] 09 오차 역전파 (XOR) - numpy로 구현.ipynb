{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc68f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 BCE Loss: 0.693982\n",
      "Epoch  1000 BCE Loss: 0.691454\n",
      "Epoch  2000 BCE Loss: 0.162601\n",
      "Epoch  3000 BCE Loss: 0.023738\n",
      "Epoch  4000 BCE Loss: 0.012139\n",
      "Epoch  5000 BCE Loss: 0.008087\n",
      "Epoch  6000 BCE Loss: 0.006045\n",
      "Epoch  7000 BCE Loss: 0.004818\n",
      "Epoch  8000 BCE Loss: 0.004002\n",
      "Epoch  9000 BCE Loss: 0.003420\n",
      "\n",
      "최종 예측 결과:\n",
      "[[0.0038]\n",
      " [0.9973]\n",
      " [0.9973]\n",
      " [0.0028]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 시그모이드 함수와 그 미분\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "# Binary Cross Entropy Loss\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    # epsilon = 1e-7  # log(0) 방지\n",
    "    # y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# XOR 입력과 출력\n",
    "x_data = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [1, 0],\n",
    "                   [1, 1]], dtype=np.float32)\n",
    "\n",
    "y_data = np.array([[0],\n",
    "                   [1],\n",
    "                   [1],\n",
    "                   [0]], dtype=np.float32)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# 가중치 초기화\n",
    "np.random.seed(0)\n",
    "W1 = np.random.uniform(-1, 1, (input_size, hidden_size))\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "W2 = np.random.uniform(-1, 1, (hidden_size, output_size))\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# 학습\n",
    "for epoch in range(epochs):\n",
    "    # 순전파\n",
    "    z1 = np.dot(x_data, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # 손실 계산 (Binary Cross Entropy)\n",
    "    loss = binary_cross_entropy(y_data, a2)\n",
    "\n",
    "    # 역전파\n",
    "    d_a2 = a2 - y_data                               # BCE + sigmoid 조합의 도함수\n",
    "    d_a1 = np.matmul(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "\n",
    "    # 가중치 업데이트\n",
    "    W2 -= np.matmul(a1.T, d_a2) * learning_rate\n",
    "    # b2 -= np.sum(d_a2, axis=0, keepdims=True) * learning_rate\n",
    "    b2 -= np.sum(d_a2, axis=0) * learning_rate\n",
    "    \n",
    "    W1 -= x_data.T.dot(d_a1) * learning_rate\n",
    "    b1 -= np.sum(d_a1, axis=0) * learning_rate\n",
    "    # b1 -= np.sum(d_a1, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    # 출력\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch:5d} BCE Loss: {loss:.6f}\")\n",
    "\n",
    "# 예측 결과\n",
    "print(\"\\n최종 예측 결과:\")\n",
    "output = sigmoid(np.dot(sigmoid(np.dot(x_data, W1) + b1), W2) + b2)\n",
    "print(np.round(output, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670af079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4acdf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c719b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20160adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################\n",
    "# XOR 문제를 해결하기 위한 2층 신경망 구현\n",
    "####################################################################\n",
    "import numpy as np\n",
    "\n",
    "# 시그모이드 함수와 그 미분\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "learning_rate = 0.1\n",
    "np.random.seed(0)\n",
    "\n",
    "# 데이터  (XOR 문제)\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "x_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd04073d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3), (3,), (3, 1), (1,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치와 바이어스 초기화\n",
    "W1 = np.random.randn(2, 3)\n",
    "b1 = np.random.randn(3)\n",
    "W2 = np.random.randn(3, 1)\n",
    "b2 = np.random.randn(1)\n",
    "W1.shape, b1.shape, W2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab09c467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (4, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward propagation\n",
    "layer1 = sigmoid(np.dot(x_data, W1) + b1)      # shape: (N, 3)\n",
    "hypothesis = sigmoid(np.dot(layer1, W2) + b2)  # shape: (N, 1)\n",
    "layer1.shape, hypothesis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb41f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0891365337429304"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비용 함수 계산\n",
    "cost = -np.mean(y_data * np.log(hypothesis) + (1 - y_data) * np.log(1 - hypothesis))\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c1f6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 1), (4, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backward propagation\n",
    "# 출력층 에러\n",
    "error = hypothesis - y_data\n",
    "d_hypothesis = error * sigmoid_derivative(hypothesis)\n",
    "error.shape, d_hypothesis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e4a490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층 에러\n",
    "layer1_error = np.dot(d_hypothesis, W2.T)\n",
    "d_layer1 = layer1_error * sigmoid_derivative(layer1)\n",
    "\n",
    "# 가중치와 바이어스 업데이트\n",
    "W2 -= learning_rate * np.dot(layer1.T, d_hypothesis)   # shape: (3, 1)\n",
    "b2 -= learning_rate * np.sum(d_hypothesis, axis=0)     # shape: (1,)\n",
    "W1 -= learning_rate * np.dot(x_data.T, d_layer1)       # shape: (2, 3)\n",
    "b1 -= learning_rate * np.sum(d_layer1, axis=0)         # shape: (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57f470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de640b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 훈련\n",
    "for step in range(10001):\n",
    "    # Forward propagation\n",
    "    layer1 = sigmoid(np.dot(x_data, W1) + b1)\n",
    "    hypothesis = sigmoid(np.dot(layer1, W2) + b2)\n",
    "    \n",
    "    # 비용 함수 계산\n",
    "    cost = -np.mean(y_data * np.log(hypothesis) + (1 - y_data) * np.log(1 - hypothesis))\n",
    "    \n",
    "    # Backward propagation\n",
    "    # 출력층 에러\n",
    "    error = hypothesis - y_data\n",
    "    d_hypothesis = error * sigmoid_derivative(hypothesis)\n",
    "    \n",
    "    # 은닉층 에러\n",
    "    layer1_error = np.dot(d_hypothesis, W2.T)\n",
    "    d_layer1 = layer1_error * sigmoid_derivative(layer1)\n",
    "    \n",
    "    # 가중치와 바이어스 업데이트\n",
    "    W2 -= learning_rate * np.dot(layer1.T, d_hypothesis)\n",
    "    b2 -= learning_rate * np.sum(d_hypothesis, axis=0)\n",
    "    W1 -= learning_rate * np.dot(x_data.T, d_layer1)\n",
    "    b1 -= learning_rate * np.sum(d_layer1, axis=0)\n",
    "    \n",
    "    # 출력\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost, W2.flatten())\n",
    "\n",
    "# 최종 결과\n",
    "layer1 = sigmoid(np.dot(x_data, W1) + b1)\n",
    "hypothesis = sigmoid(np.dot(layer1, W2) + b2)\n",
    "predicted = (hypothesis > 0.5).astype(np.float32)\n",
    "accuracy = np.mean(predicted == y_data)\n",
    "\n",
    "print(\"\\nHypothesis:\\n\", hypothesis)\n",
    "print(\"\\nCorrect:\\n\", predicted)\n",
    "print(\"\\nAccuracy:\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3554d647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d54fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce977d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "023c3c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0891365337429304 [0.39673688 0.13324823 1.44606571]\n",
      "100 0.6956167131383346 [-0.10405314 -0.29137544  1.07228891]\n",
      "200 0.6893437054152814 [ 0.03460206 -0.23916921  1.03362018]\n",
      "300 0.6836980139597717 [ 0.16995484 -0.20014552  1.02013146]\n",
      "400 0.6780877072117059 [ 0.30370312 -0.17310792  1.02786104]\n",
      "500 0.6720557667515878 [ 0.43897025 -0.15612677  1.05477467]\n",
      "600 0.6652123894670156 [ 0.57838492 -0.14797802  1.09939233]\n",
      "700 0.6572032901088167 [ 0.72419768 -0.14793976  1.16046963]\n",
      "800 0.6477236074435835 [ 0.87816361 -0.15561573  1.23673108]\n",
      "900 0.6365776190263086 [ 1.04116058 -0.17075303  1.32663466]\n",
      "1000 0.623769168667645 [ 1.21261129 -0.19304605  1.42818074]\n",
      "1100 0.6095653943128686 [ 1.39003359 -0.2219646   1.53881403]\n",
      "1200 0.5944564896460977 [ 1.56925721 -0.25667278  1.65549376]\n",
      "1300 0.579011322998094 [ 1.74551893 -0.29607494  1.77497509]\n",
      "1400 0.5637347044212916 [ 1.91483376 -0.33896465  1.89422239]\n",
      "1500 0.5490061995866804 [ 2.07476739 -0.38422307  2.01076632]\n",
      "1600 0.5350795229576236 [ 2.22439087 -0.43100351  2.12286534]\n",
      "1700 0.522098150470041 [ 2.363822   -0.47884472  2.22947471]\n",
      "1800 0.5101131393578774 [ 2.49376571 -0.52769587  2.33011026]\n",
      "1900 0.49910305054165194 [ 2.61520684 -0.57788282  2.42468866]\n",
      "2000 0.4889934668957233 [ 2.72924241 -0.63006278  2.5133886 ]\n",
      "2100 0.47967178037777486 [ 2.83700235 -0.68520553  2.5965478 ]\n",
      "2200 0.47099392406808505 [ 2.93961963 -0.74462692  2.67459585]\n",
      "2300 0.46278039875216753 [ 3.03822742 -0.81009663  2.74801785]\n",
      "2400 0.45479742003737034 [ 3.13396995 -0.88405109  2.81734575]\n",
      "2500 0.4467145310169159 [ 3.22801281 -0.96996326  2.88317942]\n",
      "2600 0.43802313068944354 [ 3.32152582 -1.07293733  2.94625077]\n",
      "2700 0.4279018626422544 [ 3.41558214 -1.20052651  3.00756434]\n",
      "2800 0.4150850822249692 [ 3.51091188 -1.36327877  3.06867553]\n",
      "2900 0.39808282864274086 [ 3.60772687 -1.57286042  3.132143  ]\n",
      "3000 0.3763381462980704 [ 3.70666158 -1.83406954  3.20185269]\n",
      "3100 0.35137907256941686 [ 3.81059635 -2.13480074  3.28216806]\n",
      "3200 0.325780518401959 [ 3.92259318 -2.44983459  3.37556582]\n",
      "3300 0.3015120118273244 [ 4.04166141 -2.75568349  3.48115801]\n",
      "3400 0.27956379636845385 [ 4.16371318 -3.03836222  3.59565596]\n",
      "3500 0.26018551809157026 [ 4.28473595 -3.29260924  3.71515875]\n",
      "3600 0.2432188639013809 [ 4.40207505 -3.51857634  3.83627252]\n",
      "3700 0.22835661781418978 [ 4.51430336 -3.71895752  3.95646218]\n",
      "3800 0.21527923839655866 [ 4.62079912 -3.89721971  4.07400729]\n",
      "3900 0.20370497468609683 [ 4.7214161  -4.05673684  4.18783296]\n",
      "4000 0.1933994244629261 [ 4.81627538 -4.20046137  4.2973342 ]\n",
      "4100 0.18417098618378544 [ 4.90564031 -4.33085796  4.40223086]\n",
      "4200 0.17586320554245227 [ 4.98984226 -4.44994339  4.50245849]\n",
      "4300 0.1683476360691345 [ 5.06923708 -4.55935808  4.59809025]\n",
      "4400 0.16151803962882588 [ 5.14418011 -4.6604384   4.68928299]\n",
      "4500 0.15528587387740023 [ 5.2150125  -4.75427898  4.77624075]\n",
      "4600 0.14957681306691978 [ 5.28205421 -4.84178313  4.85919075]\n",
      "4700 0.1443280540371428 [ 5.34560117 -4.92370241  4.93836789]\n",
      "4800 0.13948621095487623 [ 5.40592464 -5.0006674   5.01400505]\n",
      "4900 0.13500565310474946 [ 5.46327201 -5.07321142  5.08632718]\n",
      "5000 0.13084717949881072 [ 5.51786819 -5.14178906  5.15554804]\n",
      "5100 0.12697695263200093 [ 5.56991734 -5.20679058  5.22186851]\n",
      "5200 0.12336563396344279 [ 5.61960474 -5.26855323  5.28547603]\n",
      "5300 0.11998767809022592 [ 5.66709856 -5.3273703   5.34654459]\n",
      "5400 0.11682075291815908 [ 5.71255156 -5.38349823  5.40523528]\n",
      "5500 0.11384526067599669 [ 5.75610267 -5.43716249  5.4616969 ]\n",
      "5600 0.11104394020898084 [ 5.79787841 -5.4885622   5.51606678]\n",
      "5700 0.1084015351906702 [ 5.83799419 -5.53787409  5.56847162]\n",
      "5800 0.10590451609386309 [ 5.87655545 -5.58525556  5.61902835]\n",
      "5900 0.10354084622775603 [ 5.91365867 -5.63084745  5.66784489]\n",
      "6000 0.1012997840664479 [ 5.94939232 -5.67477613  5.71502092]\n",
      "6100 0.09917171559759143 [ 5.98383765 -5.71715546  5.76064864]\n",
      "6200 0.09714801160725692 [ 6.0170694  -5.75808828  5.80481334]\n",
      "6300 0.09522090576028935 [ 6.04915643 -5.7976678   5.84759406]\n",
      "6400 0.09338339008894572 [ 6.08016233 -5.83597872  5.88906413]\n",
      "6500 0.09162912510756313 [ 6.11014588 -5.87309819  5.92929162]\n",
      "6600 0.08995236225896316 [ 6.13916151 -5.90909672  5.96833987]\n",
      "6700 0.0883478767935855 [ 6.16725972 -5.94403883  6.00626779]\n",
      "6800 0.08681090950384053 [ 6.19448741 -5.97798374  6.04313034]\n",
      "6900 0.08533711599869044 [ 6.22088821 -6.01098591  6.07897877]\n",
      "7000 0.0839225224186075 [ 6.24650277 -6.04309554  6.113861  ]\n",
      "7100 0.08256348666799256 [ 6.271369   -6.07435895  6.14782184]\n",
      "7200 0.0812566643881684 [ 6.29552233 -6.10481902  6.18090324]\n",
      "7300 0.07999897901496657 [ 6.31899587 -6.13451546  6.21314455]\n",
      "7400 0.07878759536536872 [ 6.34182065 -6.16348515  6.24458269]\n",
      "7500 0.0776198962813572 [ 6.36402576 -6.19176238  6.27525237]\n",
      "7600 0.0764934619290808 [ 6.3856385  -6.21937909  6.30518622]\n",
      "7700 0.07540605141008373 [ 6.40668451 -6.24636506  6.33441495]\n",
      "7800 0.07435558639064854 [ 6.42718794 -6.27274811  6.36296753]\n",
      "7900 0.07334013649686527 [ 6.44717151 -6.29855428  6.39087126]\n",
      "8000 0.07235790625817858 [ 6.46665665 -6.32380795  6.41815194]\n",
      "8100 0.07140722341194844 [ 6.48566356 -6.34853199  6.44483395]\n",
      "8200 0.07048652840686918 [ 6.50421134 -6.37274787  6.47094033]\n",
      "8300 0.069594364964662 [ 6.52231804 -6.39647579  6.49649292]\n",
      "8400 0.06872937157787012 [ 6.54000074 -6.41973477  6.52151239]\n",
      "8500 0.06789027383735788 [ 6.55727561 -6.44254272  6.54601834]\n",
      "8600 0.06707587749665028 [ 6.57415797 -6.46491656  6.5700294 ]\n",
      "8700 0.06628506219188668 [ 6.59066236 -6.48687226  6.59356323]\n",
      "8800 0.06551677574619934 [ 6.60680259 -6.50842492  6.61663662]\n",
      "8900 0.0647700289959985 [ 6.62259175 -6.52958883  6.63926556]\n",
      "9000 0.06404389108415165 [ 6.63804233 -6.55037755  6.66146524]\n",
      "9100 0.06333748517155632 [ 6.65316618 -6.5708039   6.68325015]\n",
      "9200 0.06264998452427115 [ 6.66797458 -6.59088008  6.70463409]\n",
      "9300 0.061980608938299195 [ 6.68247829 -6.61061768  6.72563021]\n",
      "9400 0.061328621468424854 [ 6.69668757 -6.63002771  6.74625108]\n",
      "9500 0.060693325431269846 [ 6.71061219 -6.64912064  6.76650868]\n",
      "9600 0.06007406165603192 [ 6.7242615  -6.66790647  6.78641448]\n",
      "9700 0.059470205959263334 [ 6.73764441 -6.68639469  6.80597942]\n",
      "9800 0.05888116682259553 [ 6.75076943 -6.70459439  6.82521399]\n",
      "9900 0.058306383254551654 [ 6.76364473 -6.72251424  6.8441282 ]\n",
      "10000 0.05774532281957198 [ 6.77627809 -6.7401625   6.86273165]\n",
      "\n",
      "Hypothesis:\n",
      " [[0.04482366]\n",
      " [0.93787025]\n",
      " [0.95057976]\n",
      " [0.06786063]]\n",
      "\n",
      "Correct:\n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "\n",
      "Accuracy:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "# XOR 문제를 해결하기 위한 2층 신경망 구현\n",
    "####################################################################\n",
    "import numpy as np\n",
    "\n",
    "# 시그모이드 함수와 그 미분\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "learning_rate = 0.1\n",
    "np.random.seed(0)\n",
    "\n",
    "# 데이터  (XOR 문제)\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# 가중치와 바이어스 초기화\n",
    "W1 = np.random.normal(size=(2, 3))\n",
    "b1 = np.random.normal(size=(3))\n",
    "W2 = np.random.normal(size=(3, 1))\n",
    "b2 = np.random.normal(size=(1))\n",
    "\n",
    "# 훈련\n",
    "for step in range(10001):\n",
    "    # Forward propagation\n",
    "    layer1 = sigmoid(np.dot(x_data, W1) + b1)\n",
    "    hypothesis = sigmoid(np.dot(layer1, W2) + b2)\n",
    "    \n",
    "    # 비용 함수 계산\n",
    "    cost = -np.mean(y_data * np.log(hypothesis) + (1 - y_data) * np.log(1 - hypothesis))\n",
    "    \n",
    "    # Backward propagation\n",
    "    # 출력층 에러\n",
    "    error = hypothesis - y_data\n",
    "    d_hypothesis = error * sigmoid_derivative(hypothesis)\n",
    "    \n",
    "    # 은닉층 에러\n",
    "    layer1_error = np.dot(d_hypothesis, W2.T)\n",
    "    d_layer1 = layer1_error * sigmoid_derivative(layer1)\n",
    "    \n",
    "    # 가중치와 바이어스 업데이트\n",
    "    W2 -= learning_rate * np.dot(layer1.T, d_hypothesis)\n",
    "    b2 -= learning_rate * np.sum(d_hypothesis, axis=0)\n",
    "    W1 -= learning_rate * np.dot(x_data.T, d_layer1)\n",
    "    b1 -= learning_rate * np.sum(d_layer1, axis=0)\n",
    "    \n",
    "    # 출력\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost, W2.flatten())\n",
    "\n",
    "# 최종 결과\n",
    "layer1 = sigmoid(np.dot(x_data, W1) + b1)\n",
    "hypothesis = sigmoid(np.dot(layer1, W2) + b2)\n",
    "predicted = (hypothesis > 0.5).astype(np.float32)\n",
    "accuracy = np.mean(predicted == y_data)\n",
    "\n",
    "print(\"\\nHypothesis:\\n\", hypothesis)\n",
    "print(\"\\nCorrect:\\n\", predicted)\n",
    "print(\"\\nAccuracy:\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a1fedc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{a e^{- a x - b}}{\\left(e^{- a x - b} + 1\\right)^{2}}$"
      ],
      "text/plain": [
       "a*exp(-a*x - b)/(exp(-a*x - b) + 1)**2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp \n",
    "\n",
    "a, b, x = sp.symbols('a b x')\n",
    "sigmoid = 1 / (1 + sp.exp(-(a * x + b)))\n",
    "\n",
    "sp.Derivative(sigmoid, x).doit()  # 시그모이드 함수의 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059d1a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,1) and (4,2) not aligned: 1 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m d_layer1 \u001b[38;5;241m=\u001b[39m layer1_error \u001b[38;5;241m*\u001b[39m sigmoid_derivative(layer1)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# 가중치와 바이어스 업데이트\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# W3 -= learning_rate * np.dot(layer2.T, d_hypothesis)\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m W3 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_hypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m b3 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(d_hypothesis, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     56\u001b[0m W2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(layer1\u001b[38;5;241m.\u001b[39mT, d_layer2)\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (4,1) and (4,2) not aligned: 1 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 시그모이드 함수와 그 미분\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "learning_rate = 0.1\n",
    "np.random.seed(0)\n",
    "\n",
    "# 데이터\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# 가중치와 바이어스 초기화\n",
    "# 입력층 -> 첫 번째 히든 레이어 (2 -> 4)\n",
    "W1 = np.random.normal(size=(2, 4))\n",
    "b1 = np.random.normal(size=(4))\n",
    "# 첫 번째 히든 레이어 -> 두 번째 히든 레이어 (4 -> 2)\n",
    "W2 = np.random.normal(size=(4, 2))\n",
    "b2 = np.random.normal(size=(2))\n",
    "# 두 번째 히든 레이어 -> 출력층 (2 -> 1)\n",
    "W3 = np.random.normal(size=(2, 1))\n",
    "b3 = np.random.normal(size=(1))\n",
    "\n",
    "# 훈련\n",
    "for step in range(10001):\n",
    "    # Forward propagation\n",
    "    layer1 = sigmoid(np.dot(x_data, W1) + b1)  # 첫 번째 히든 레이어\n",
    "    layer2 = sigmoid(np.dot(layer1, W2) + b2)  # 두 번째 히든 레이어\n",
    "    hypothesis = sigmoid(np.dot(layer2, W3) + b3)  # 출력층\n",
    "    \n",
    "    # 비용 함수 계산 (Binary Cross-Entropy)\n",
    "    cost = -np.mean(y_data * np.log(hypothesis) + (1 - y_data) * np.log(1 - hypothesis))\n",
    "    \n",
    "    # Backward propagation\n",
    "    # 출력층 에러\n",
    "    error = hypothesis - y_data\n",
    "    d_hypothesis = error * sigmoid_derivative(hypothesis)\n",
    "    \n",
    "    # 두 번째 히든 레이어 에러\n",
    "    layer2_error = np.dot(d_hypothesis, W3.T)\n",
    "    d_layer2 = layer2_error * sigmoid_derivative(layer2)\n",
    "    \n",
    "    # 첫 번째 히든 레이어 에러\n",
    "    layer1_error = np.dot(d_layer2, W2.T)\n",
    "    d_layer1 = layer1_error * sigmoid_derivative(layer1)\n",
    "    \n",
    "    # 가중치와 바이어스 업데이트\n",
    "    W3 -= learning_rate * np.dot(layer2.T, d_hypothesis)\n",
    "    b3 -= learning_rate * np.sum(d_hypothesis, axis=0)\n",
    "    W2 -= learning_rate * np.dot(layer1.T, d_layer2)\n",
    "    b2 -= learning_rate * np.sum(d_layer2, axis=0)\n",
    "    W1 -= learning_rate * np.dot(x_data.T, d_layer1)\n",
    "    b1 -= learning_rate * np.sum(d_layer1, axis=0)\n",
    "    \n",
    "    # 출력\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost, W3.flatten())\n",
    "\n",
    "# 최종 결과\n",
    "layer1 = sigmoid(np.dot(x_data, W1) + b1)\n",
    "layer2 = sigmoid(np.dot(layer1, W2) + b2)\n",
    "hypothesis = sigmoid(np.dot(layer2, W3) + b3)\n",
    "predicted = (hypothesis > 0.5).astype(np.float32)\n",
    "accuracy = np.mean(predicted == y_data)\n",
    "\n",
    "print(\"\\nHypothesis:\\n\", hypothesis)\n",
    "print(\"\\nCorrect:\\n\", predicted)\n",
    "print(\"\\nAccuracy:\\n\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc78d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
